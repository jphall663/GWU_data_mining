{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License \n",
    "***\n",
    "Copyright (C) 2018 J. Patrick Hall, jphall@gwu.edu\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Simple multilayer perception (MLP) example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import urllib.request as urllib2\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set simple hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARN_RATE = 0.0005\n",
    "ITERATIONS = 300\n",
    "HIDDEN_UNITS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch simple Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inputs:\n",
      " [[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]\n",
      " [ 5.4  3.9  1.7  0.4]\n",
      " [ 4.6  3.4  1.4  0.3]\n",
      " [ 5.   3.4  1.5  0.2]\n",
      " [ 4.4  2.9  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.4  3.7  1.5  0.2]\n",
      " [ 4.8  3.4  1.6  0.2]\n",
      " [ 4.8  3.   1.4  0.1]\n",
      " [ 4.3  3.   1.1  0.1]\n",
      " [ 5.8  4.   1.2  0.2]\n",
      " [ 5.7  4.4  1.5  0.4]\n",
      " [ 5.4  3.9  1.3  0.4]\n",
      " [ 5.1  3.5  1.4  0.3]\n",
      " [ 5.7  3.8  1.7  0.3]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 5.4  3.4  1.7  0.2]\n",
      " [ 5.1  3.7  1.5  0.4]\n",
      " [ 4.6  3.6  1.   0.2]\n",
      " [ 5.1  3.3  1.7  0.5]\n",
      " [ 4.8  3.4  1.9  0.2]\n",
      " [ 5.   3.   1.6  0.2]\n",
      " [ 5.   3.4  1.6  0.4]\n",
      " [ 5.2  3.5  1.5  0.2]\n",
      " [ 5.2  3.4  1.4  0.2]\n",
      " [ 4.7  3.2  1.6  0.2]\n",
      " [ 4.8  3.1  1.6  0.2]\n",
      " [ 5.4  3.4  1.5  0.4]\n",
      " [ 5.2  4.1  1.5  0.1]\n",
      " [ 5.5  4.2  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.   3.2  1.2  0.2]\n",
      " [ 5.5  3.5  1.3  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 4.4  3.   1.3  0.2]\n",
      " [ 5.1  3.4  1.5  0.2]\n",
      " [ 5.   3.5  1.3  0.3]\n",
      " [ 4.5  2.3  1.3  0.3]\n",
      " [ 4.4  3.2  1.3  0.2]\n",
      " [ 5.   3.5  1.6  0.6]\n",
      " [ 5.1  3.8  1.9  0.4]\n",
      " [ 4.8  3.   1.4  0.3]\n",
      " [ 5.1  3.8  1.6  0.2]\n",
      " [ 4.6  3.2  1.4  0.2]\n",
      " [ 5.3  3.7  1.5  0.2]\n",
      " [ 5.   3.3  1.4  0.2]\n",
      " [ 7.   3.2  4.7  1.4]\n",
      " [ 6.4  3.2  4.5  1.5]\n",
      " [ 6.9  3.1  4.9  1.5]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 6.5  2.8  4.6  1.5]\n",
      " [ 5.7  2.8  4.5  1.3]\n",
      " [ 6.3  3.3  4.7  1.6]\n",
      " [ 4.9  2.4  3.3  1. ]\n",
      " [ 6.6  2.9  4.6  1.3]\n",
      " [ 5.2  2.7  3.9  1.4]\n",
      " [ 5.   2.   3.5  1. ]\n",
      " [ 5.9  3.   4.2  1.5]\n",
      " [ 6.   2.2  4.   1. ]\n",
      " [ 6.1  2.9  4.7  1.4]\n",
      " [ 5.6  2.9  3.6  1.3]\n",
      " [ 6.7  3.1  4.4  1.4]\n",
      " [ 5.6  3.   4.5  1.5]\n",
      " [ 5.8  2.7  4.1  1. ]\n",
      " [ 6.2  2.2  4.5  1.5]\n",
      " [ 5.6  2.5  3.9  1.1]\n",
      " [ 5.9  3.2  4.8  1.8]\n",
      " [ 6.1  2.8  4.   1.3]\n",
      " [ 6.3  2.5  4.9  1.5]\n",
      " [ 6.1  2.8  4.7  1.2]\n",
      " [ 6.4  2.9  4.3  1.3]\n",
      " [ 6.6  3.   4.4  1.4]\n",
      " [ 6.8  2.8  4.8  1.4]\n",
      " [ 6.7  3.   5.   1.7]\n",
      " [ 6.   2.9  4.5  1.5]\n",
      " [ 5.7  2.6  3.5  1. ]\n",
      " [ 5.5  2.4  3.8  1.1]\n",
      " [ 5.5  2.4  3.7  1. ]\n",
      " [ 5.8  2.7  3.9  1.2]\n",
      " [ 6.   2.7  5.1  1.6]\n",
      " [ 5.4  3.   4.5  1.5]\n",
      " [ 6.   3.4  4.5  1.6]\n",
      " [ 6.7  3.1  4.7  1.5]\n",
      " [ 6.3  2.3  4.4  1.3]\n",
      " [ 5.6  3.   4.1  1.3]\n",
      " [ 5.5  2.5  4.   1.3]\n",
      " [ 5.5  2.6  4.4  1.2]\n",
      " [ 6.1  3.   4.6  1.4]\n",
      " [ 5.8  2.6  4.   1.2]\n",
      " [ 5.   2.3  3.3  1. ]\n",
      " [ 5.6  2.7  4.2  1.3]\n",
      " [ 5.7  3.   4.2  1.2]\n",
      " [ 5.7  2.9  4.2  1.3]\n",
      " [ 6.2  2.9  4.3  1.3]\n",
      " [ 5.1  2.5  3.   1.1]\n",
      " [ 5.7  2.8  4.1  1.3]]\n",
      "\n",
      "\n",
      "Data target:\n",
      " [[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "# load and preprocess Iris data set \n",
    "# easy binomial classification task: seperate Setosa irises from Versicolor irises\n",
    "\n",
    "# fetch data from UCI repository\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "connection = urllib2.urlopen(url)\n",
    "raw = connection.read()\n",
    "\n",
    "# initialize empty X and y arrays\n",
    "X = np.zeros((100, 4))\n",
    "y = np.zeros((100, 1))\n",
    "\n",
    "# load iris data into X and y arrays \n",
    "row_idx = 0\n",
    "for line in str(raw)[2:-5].split('\\\\n'):\n",
    "    line = line.replace('Iris-setosa', '1').replace('Iris-versicolor', '0')\n",
    "    line = line.split(',')\n",
    "    # remove Virginica irises from data set\n",
    "    if line[-1] != 'Iris-virginica':\n",
    "        line = np.asarray(line)\n",
    "        X[row_idx, :] = line[:-1]\n",
    "        y[row_idx, :] = line[-1]\n",
    "        row_idx += 1\n",
    "\n",
    "        \n",
    "print('Data inputs:\\n', X)\n",
    "print('\\n')\n",
    "print('Data target:\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP architecture is: 4 input units -> 30 hidden units -> 1 output units.\n",
      "\n",
      "There are 120 hidden weights to optimize.\n",
      "Initial hidden weights:\n",
      " [[  4.29616093e-01  -1.83624445e-01  -3.16081188e-01  -2.95439721e-01\n",
      "    6.77250291e-02   9.55447030e-02   4.64514520e-01   1.53177097e-01\n",
      "    2.48906638e-01   1.53569871e-01   2.47714809e-01   4.61306736e-01\n",
      "   -4.91611702e-01  -3.93555623e-01  -2.01296286e-01   1.56411183e-01\n",
      "    3.09812553e-01   3.72175914e-01   4.64647597e-01   2.23685347e-01\n",
      "    1.42475328e-01   2.17453621e-01  -3.24009928e-02  -1.74415322e-01\n",
      "   -6.03553941e-02   2.29689083e-01   4.94014586e-01   1.76873712e-01\n",
      "    2.90822518e-01  -3.29085742e-01]\n",
      " [ -4.73150724e-01   3.00370244e-01   4.03722538e-01  -4.75323790e-01\n",
      "   -8.25268155e-03   2.62551673e-02   9.63660104e-02  -4.48042455e-01\n",
      "    3.95089528e-01   2.28266180e-01   3.18350011e-01   2.22752834e-04\n",
      "    3.10189409e-01  -4.04031474e-01  -2.81049956e-01  -2.41280938e-01\n",
      "   -3.18942460e-02  -4.06267974e-02   2.09509780e-01  -3.21946994e-01\n",
      "    3.14498844e-02  -3.32257771e-01   2.68813918e-01   4.28170549e-01\n",
      "    1.09493658e-01  -3.49816505e-01  -1.03732963e-02  -1.22655046e-01\n",
      "    3.48601412e-01   4.11097229e-01]\n",
      " [ -1.16151279e-01  -1.84504097e-01   6.83941528e-02  -3.12181965e-01\n",
      "   -3.74158456e-01   1.87595805e-01   2.99606718e-01   7.35365652e-02\n",
      "    4.73229982e-01   1.34054377e-01   3.88421725e-01  -4.58524124e-03\n",
      "   -1.48383470e-01   2.14230369e-01   3.92911645e-03  -2.74362393e-01\n",
      "   -2.55025560e-01   2.92800700e-01  -4.82758549e-03   4.15093673e-01\n",
      "    4.45371834e-01   3.32322297e-02  -2.47507405e-01   2.20862058e-01\n",
      "   -1.32561236e-01  -1.35155709e-03  -2.73424953e-01  -1.46434353e-01\n",
      "    1.50851787e-01  -1.87067105e-01]\n",
      " [  2.68735447e-01   2.81837103e-01   3.52409483e-01   4.49905740e-01\n",
      "   -3.92677088e-01   4.10725356e-01  -1.63944838e-01   3.26380427e-01\n",
      "    3.98100635e-01  -4.57284696e-01  -3.04205001e-01  -2.05498678e-01\n",
      "    1.26999881e-01  -4.13776895e-01  -3.57054980e-01   1.58265192e-02\n",
      "    1.89341330e-01   3.56625811e-01   1.47361683e-01   8.16186755e-02\n",
      "    2.11115955e-01  -2.47583143e-01   4.00159683e-01  -5.77063071e-02\n",
      "   -4.79479175e-01   4.59661014e-01   1.52225422e-01   1.32062501e-02\n",
      "    1.82356383e-01  -1.04596094e-02]]\n",
      "\n",
      "There are 30 output weights to optimize.\n",
      "Initial output weights:\n",
      " [[ 0.42649017]\n",
      " [ 0.01587977]\n",
      " [-0.42784012]\n",
      " [ 0.0675083 ]\n",
      " [ 0.11524318]\n",
      " [ 0.44154629]\n",
      " [-0.08463665]\n",
      " [-0.23556003]\n",
      " [-0.40260683]\n",
      " [-0.01415578]\n",
      " [-0.03533714]\n",
      " [-0.47024068]\n",
      " [ 0.19427746]\n",
      " [ 0.21694711]\n",
      " [ 0.22981142]\n",
      " [-0.08564898]\n",
      " [-0.48490116]\n",
      " [ 0.40897516]\n",
      " [ 0.28937872]\n",
      " [-0.33480083]\n",
      " [-0.18721404]\n",
      " [ 0.11094531]\n",
      " [-0.13550971]\n",
      " [-0.34396141]\n",
      " [-0.32269619]\n",
      " [ 0.36788967]\n",
      " [-0.20990533]\n",
      " [ 0.08517962]\n",
      " [-0.04600512]\n",
      " [-0.08882187]]\n",
      "\n",
      "Initial yhat:\n",
      " [[ 0.24753092]\n",
      " [ 0.25931333]\n",
      " [ 0.25272927]\n",
      " [ 0.25386176]\n",
      " [ 0.2440762 ]\n",
      " [ 0.24032274]\n",
      " [ 0.24705247]\n",
      " [ 0.24888208]\n",
      " [ 0.25845649]\n",
      " [ 0.25535009]\n",
      " [ 0.24460842]\n",
      " [ 0.24689153]\n",
      " [ 0.25774061]\n",
      " [ 0.25550082]\n",
      " [ 0.24242968]\n",
      " [ 0.23189508]\n",
      " [ 0.24214685]\n",
      " [ 0.24835111]\n",
      " [ 0.24486826]\n",
      " [ 0.24023051]\n",
      " [ 0.25156733]\n",
      " [ 0.24358553]\n",
      " [ 0.24275981]\n",
      " [ 0.25381345]\n",
      " [ 0.2458832 ]\n",
      " [ 0.25925736]\n",
      " [ 0.25005186]\n",
      " [ 0.24796707]\n",
      " [ 0.25104261]\n",
      " [ 0.25147818]\n",
      " [ 0.25494329]\n",
      " [ 0.25393003]\n",
      " [ 0.23195235]\n",
      " [ 0.23350859]\n",
      " [ 0.25535009]\n",
      " [ 0.25562701]\n",
      " [ 0.25168511]\n",
      " [ 0.25535009]\n",
      " [ 0.25609123]\n",
      " [ 0.24972576]\n",
      " [ 0.24797449]\n",
      " [ 0.2776847 ]\n",
      " [ 0.25058758]\n",
      " [ 0.24903445]\n",
      " [ 0.23953033]\n",
      " [ 0.25930852]\n",
      " [ 0.23899369]\n",
      " [ 0.25154877]\n",
      " [ 0.24368678]\n",
      " [ 0.25197602]\n",
      " [ 0.27088801]\n",
      " [ 0.26591219]\n",
      " [ 0.27252074]\n",
      " [ 0.28258432]\n",
      " [ 0.27653197]\n",
      " [ 0.27113881]\n",
      " [ 0.26290388]\n",
      " [ 0.27584151]\n",
      " [ 0.27499745]\n",
      " [ 0.27040464]\n",
      " [ 0.28762903]\n",
      " [ 0.26713655]\n",
      " [ 0.28820243]\n",
      " [ 0.27145341]\n",
      " [ 0.26738991]\n",
      " [ 0.270671  ]\n",
      " [ 0.26555881]\n",
      " [ 0.27362861]\n",
      " [ 0.28954755]\n",
      " [ 0.27758806]\n",
      " [ 0.26282735]\n",
      " [ 0.27338843]\n",
      " [ 0.28309697]\n",
      " [ 0.27404529]\n",
      " [ 0.27322464]\n",
      " [ 0.27231577]\n",
      " [ 0.27909192]\n",
      " [ 0.27316868]\n",
      " [ 0.2705194 ]\n",
      " [ 0.27533592]\n",
      " [ 0.27959107]\n",
      " [ 0.27939121]\n",
      " [ 0.27374183]\n",
      " [ 0.27643558]\n",
      " [ 0.26447254]\n",
      " [ 0.25827335]\n",
      " [ 0.27077494]\n",
      " [ 0.28781218]\n",
      " [ 0.26491407]\n",
      " [ 0.27725351]\n",
      " [ 0.27499742]\n",
      " [ 0.26879769]\n",
      " [ 0.27640164]\n",
      " [ 0.27914974]\n",
      " [ 0.27275717]\n",
      " [ 0.26548443]\n",
      " [ 0.26819554]\n",
      " [ 0.27172772]\n",
      " [ 0.27471698]\n",
      " [ 0.27069514]]\n",
      "\n",
      "Training ...\n",
      "Iteration  100, Error:  1.93\n",
      "Iteration  200, Error:  0.73\n",
      "Iteration  300, Error:  0.43\n",
      "Maximum iterations reached, done.\n"
     ]
    }
   ],
   "source": [
    "# very simple MLP routine\n",
    "# with logistic activation for hidden and output layer\n",
    "\n",
    "# set random seed\n",
    "# always do this when working with random numbers\n",
    "np.random.seed(12345)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "hidden_weights = np.random.random((4, HIDDEN_UNITS)) - 0.5 # 4 X HIDDEN_UNITS weights in hidden layer\n",
    "output_weights = np.random.random((HIDDEN_UNITS, 1)) - 0.5 # HIDDEN_UNITS X 1 weights in output layer\n",
    "\n",
    "print('MLP architecture is: 4 input units -> %d hidden units -> 1 output units.' % HIDDEN_UNITS)\n",
    "print()\n",
    "print('There are %d hidden weights to optimize.' % (4 * HIDDEN_UNITS))\n",
    "print('Initial hidden weights:\\n', hidden_weights)\n",
    "print()\n",
    "print('There are %d output weights to optimize.' % HIDDEN_UNITS)\n",
    "print('Initial output weights:\\n', output_weights)\n",
    "\n",
    "# initialize empty pandas DataFrame to hold iteration scores\n",
    "iter_frame = pd.DataFrame(columns=['Iteration', 'Error'])\n",
    "\n",
    "# activation function\n",
    "def logistic_activation_function(weights_times_inputs):\n",
    "\n",
    "    return 1 / (1 + np.exp(-weights_times_inputs))\n",
    "\n",
    "# trainign loop\n",
    "for iteration in range(0, ITERATIONS):\n",
    "\n",
    "    ### feed-forward phase ##########\n",
    "    # run data through input, hidden, and output layers\n",
    "    \n",
    "    input_layer = X\n",
    "    hidden_layer = logistic_activation_function(np.dot(input_layer, hidden_weights))\n",
    "    output_layer = logistic_activation_function(np.dot(hidden_layer, output_weights))\n",
    "    \n",
    "    if iteration == 0:\n",
    "        print('\\nInitial yhat:\\n', output_layer)\n",
    "        print()    \n",
    "        print('Training ...')\n",
    "            \n",
    "    ### evaluate error function ##########\n",
    "    output_logloss_error = -y * np.log(output_layer) + (1 - y)*np.log(1 - output_layer)\n",
    "    if ((iteration + 1) % 100) == 0:\n",
    "        print('Iteration %4i, Error: %5.2f' % (iteration + 1, np.sum(output_logloss_error)))\n",
    "    \n",
    "    # record iteration and error\n",
    "    iter_frame = iter_frame.append({'Iteration': iteration,\n",
    "                                    'Error': np.sum(output_logloss_error)}, \n",
    "                                   ignore_index=True)    \n",
    "        \n",
    "    ### back-propogation phase ##########\n",
    "    # back-propogate error from output layer to hidden layer \n",
    "    # weight's output delta and input activation are multiplied to find the gradient of the weights\n",
    "    # due to chain rule\n",
    "    output_layer_delta = output_layer - y                          \n",
    "    hidden_layer_delta = output_layer_delta.dot(output_weights.T)  \n",
    "    output_layer_gradient = hidden_layer.T.dot(output_layer_delta) \n",
    "    hidden_layer_gradient = input_layer.T.dot(hidden_layer_delta)\n",
    "    \n",
    "    ### update weights based on gradient ##########\n",
    "    # update weights in direction that minimizes error using layerwise gradients\n",
    "    # (input layer is never updated, b/c it is the data itself)\n",
    "    # scale by learning rate\n",
    "    output_weights -= LEARN_RATE * output_layer_gradient\n",
    "    hidden_weights -= LEARN_RATE * hidden_layer_gradient\n",
    "\n",
    "print('Maximum iterations reached, done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>yhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.941330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.964082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.928115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.948628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.958275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.927787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.938498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.936620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.988920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.981388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.924859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.879935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.924512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.917590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y      yhat\n",
       "0   1.0  0.968589\n",
       "1   1.0  0.941330\n",
       "2   1.0  0.964082\n",
       "3   1.0  0.928115\n",
       "4   1.0  0.971731\n",
       "5   1.0  0.948628\n",
       "6   1.0  0.958275\n",
       "7   1.0  0.953249\n",
       "8   1.0  0.927787\n",
       "9   1.0  0.938498\n",
       "10  1.0  0.968641\n",
       "11  1.0  0.936620\n",
       "12  1.0  0.946393\n",
       "13  1.0  0.975073\n",
       "14  1.0  0.988920\n",
       "15  1.0  0.982535\n",
       "16  1.0  0.981388\n",
       "17  1.0  0.965206\n",
       "18  1.0  0.950057\n",
       "19  1.0  0.968045\n",
       "20  1.0  0.924859\n",
       "21  1.0  0.960236\n",
       "22  1.0  0.989257\n",
       "23  1.0  0.879935\n",
       "24  1.0  0.857617\n",
       "25  1.0  0.900685\n",
       "26  1.0  0.924512\n",
       "27  1.0  0.959576\n",
       "28  1.0  0.965102\n",
       "29  1.0  0.917590\n",
       "..  ...       ...\n",
       "70  0.0  0.032641\n",
       "71  0.0  0.052267\n",
       "72  0.0  0.028500\n",
       "73  0.0  0.034247\n",
       "74  0.0  0.046388\n",
       "75  0.0  0.045833\n",
       "76  0.0  0.034859\n",
       "77  0.0  0.031664\n",
       "78  0.0  0.036816\n",
       "79  0.0  0.073014\n",
       "80  0.0  0.047718\n",
       "81  0.0  0.052907\n",
       "82  0.0  0.052158\n",
       "83  0.0  0.026683\n",
       "84  0.0  0.034921\n",
       "85  0.0  0.043435\n",
       "86  0.0  0.039332\n",
       "87  0.0  0.034652\n",
       "88  0.0  0.048135\n",
       "89  0.0  0.041316\n",
       "90  0.0  0.034515\n",
       "91  0.0  0.037221\n",
       "92  0.0  0.046328\n",
       "93  0.0  0.064892\n",
       "94  0.0  0.039875\n",
       "95  0.0  0.046623\n",
       "96  0.0  0.043800\n",
       "97  0.0  0.044721\n",
       "98  0.0  0.099820\n",
       "99  0.0  0.045042\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_yhat_frame = pd.DataFrame(columns = ['y', 'yhat'])\n",
    "y_yhat_frame['y'] = y.reshape(-1)\n",
    "y_yhat_frame['yhat'] = output_layer.reshape(-1)\n",
    "y_yhat_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAGHCAYAAABVt+ARAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVNAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114cf9f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "_ = iter_frame.plot(kind='line', x='Iteration', y='Error', title='Iteration Chart')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
